# -*- coding: utf-8 -*-
"""pix2pixGAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LiVVwxr5UJTtKfvnQrWE9ZkogU2Tn-0O
"""

# pix2pix GAN model
import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras.models import Model,Sequential
from keras.layers import Input,Conv2D,Conv2DTranspose,LeakyReLU,Activation,Concatenate,BatchNormalization,Dropout,Activation,Reshape
import matplotlib.pyplot as plt
from tensorflow.keras.utils import plot_model
from keras.optimizers import Adam
from keras.initializers import RandomNormal
from keras.preprocessing.image import load_img,img_to_array
from numpy import asarray
import os

!kaggle datasets download -d vikramtiwari/pix2pix-dataset

import zipfile
file_name='/content/pix2pix-dataset.zip'
with zipfile.ZipFile(file_name,'r') as zip:
  zip.extractall()
  print('done')

path='/content/maps/maps/train'

import cv2
im=cv2.imread('/content/cityscapes/cityscapes/train/1.jpg')
print(im.shape)

def load_images(path, size=(256,512)):
  src_list, tar_list = [], []
  # Get a list of all files in the directory
  for filename in os.listdir(path):
    # Check if the file is an image (you might need to adjust the extension check)
    if filename.endswith(('.jpg', '.jpeg', '.png')):
      # Construct the full path to the image
      full_path = os.path.join(path, filename)
      # Load and process the image
      pixels = load_img(full_path, target_size=size)
      pixels = img_to_array(pixels)
      sat_img, map_img = pixels[:, :256], pixels[:, 256:]
      src_list.append(sat_img)
      tar_list.append(map_img)
  return [asarray(src_list), asarray(tar_list)]

[src_images,tar_images]=load_images(path)
print('Loaded: ',src_images.shape,tar_images.shape)

n_samples=3

for i in range(n_samples):
  plt.subplot(2,n_samples,1+i)
  plt.axis('Off')
  plt.imshow(src_images[i].astype('uint8'))

for i in range(n_samples):
  plt.subplot(2,n_samples,1+n_samples+i)
  plt.axis('Off')
  plt.imshow(tar_images[i].astype('uint8'))
plt.show()

data=[src_images,tar_images]

def preprocess_data(data):
  X1,X2=data
  X1=X1.astype('float32')
  X2=X2.astype('float32')
  X1=X1/127.5-1.0
  X2=X2/127.5-1.0
  return [X1,X2]

dataset=preprocess_data(data)

"""# Decoder Model -PatchGAN
C64-C128-C256-C512

After the last layer, a convolution is applied to map to


a 1-dimensional output, followed by a Sigmoid function.

PatchGAN discriminator for the Pix2Pix model, where the architecture focuses on small patches of the input images to determine whether the target image is real or fake based on the input source image. The concatenation of source and target images, followed by several convolutional layers, allows the discriminator to learn local features and patterns.
"""

def define_discriminator(image_shape):

  init=RandomNormal(stddev=0.02)


  in_src_image=Input(shape=image_shape)
  in_target_image=Input(shape=image_shape)

  merge=Concatenate()([in_src_image,in_target_image])

  d=Conv2D(64,(4,4),strides=(2,2),padding='same',kernel_initializer=init)(merge)
  d=LeakyReLU(alpha=0.2)(d)

  d=Conv2D(128,(4,4),strides=(2,2),padding='same',kernel_initializer=init)(d)
  d=BatchNormalization()(d)
  d=LeakyReLU(alpha=0.2)(d)

  d=Conv2D(256,(4,4),strides=(2,2),padding='same',kernel_initializer=init)(d)
  d=BatchNormalization()(d)
  d=LeakyReLU(alpha=0.2)(d)

  d=Conv2D(512,(4,4),padding='same',kernel_initializer=init)(d)
  d=BatchNormalization()(d)
  d=LeakyReLU(alpha=0.2)(d)

  #patch output
  d=Conv2D(1,(4,4),padding='same',kernel_initializer=init)(d)
  patch_out=Activation('sigmoid')(d)

  model=Model([in_src_image,in_target_image],patch_out)

  opt=Adam(learning_rate=0.0002,beta_1=0.5)

  model.compile(loss='binary_crossentropy',optimizer=opt,loss_weights=0.5)
  return model

d_model=define_discriminator((256,256,3))
plot_model(d_model,'disc.png',show_shapes=True)

"""# Generator Model -(U-Net Architecture)

#U-Net Architecture
U-Net is a popular architecture based on the encoder-decoder structure but enhanced with skip connections. U-Net was initially designed for biomedical image segmentation, where the input is an image, and the output is a segmented version of the image, labeling pixels as belonging to different regions or objects.
"""

def define_encoder_block(layer_in, n_filters, batchnorm=True):
  # weight initializer
  init=RandomNormal(stddev=0.002)
  # add downsampling layer
  g=Conv2D(n_filters,(4,4),strides=(2,2),padding='same',kernel_initializer=init)(layer_in)
  # conditionally add batch Normalization

  if batchnorm :
    g=BatchNormalization()(g,training=True)
  #leaky relu activation # Moved outside the 'if' block
  g=LeakyReLU(alpha=0.2)(g)
  return g # Now returns 'g' regardless of 'batchnorm'

"""**Downsampling**: \The main function of this block is to downsample the input by reducing its spatial resolution and increasing the number of feature channels.

 **Feature Extraction**: \The convolutional layers learn and extract feature maps from the input data, which are crucial for high-level tasks like segmentation or image translation.

 **Batch Normalization**: \Stabilizes the training process by normalizing the activations.

 **Leaky ReLU**: \Adds non-linearity to the model while preventing the "dying ReLU" problem by allowing small negative values to pass through.

# Decoder block
 This block is responsible for upsampling the feature map, restoring the spatial resolution, and merging with the corresponding feature map from the encoder path via a skip connection. This process helps the decoder recover fine-grained details that were lost during downsampling in the encoder.


 **Upsampling**: The main function of this block is to upsample the feature maps, restoring the spatial dimensions that were reduced in the encoder.

**Feature Extraction**: The transposed convolution learns to generate higher-resolution feature maps that help in reconstructing the original image or target output.

**Skip Connection**: The skip connection merges fine-grained details from the encoder with the upsampled feature maps, allowing the decoder to combine both high-level and low-level features.

**Dropout and Batch Normalizatio**n: These help prevent overfitting and stabilize the training process, respectively.
"""

def decoder_block(layer_in, skip_in, n_filters, dropout=True):
    # weight initialization
    init = RandomNormal(stddev=0.02)

    # add upsampling layer
    g = Conv2DTranspose(n_filters, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(layer_in)

    # add batch Normalization
    g = BatchNormalization()(g, training=True)

    # conditionally add dropout
    if dropout:
        g = Dropout(0.5)(g)

    # merge with skip connection
    g = Concatenate()([g, skip_in])

    # add relu activation
    g = Activation('relu')(g)

    return g

"""The skip connection is crucial because it allows the model to preserve and reuse fine-grained details from the encoder that might have been lost during downsampling. By concatenating the encoder's features directly into the decoder, the network learns both the global structure (from the deeper layers) and the finer details (from the encoder).

# GENERATOR:

**Encodes the Input Image**: The first part of the network is an encoder that processes the input image by applying multiple convolutional layers. Each layer downsamples the image, reducing its resolution but capturing important features. This step progressively compresses the information while retaining essential characteristics of the image.

**Applies the Bottleneck**: After encoding, the image reaches a "bottleneck" layer where it's represented as a highly compressed feature map. This is the deepest and most abstract representation of the image's content.

**Decodes to Reconstruct the Output Image**: The second part of the network is a decoder that takes the compressed representation from the bottleneck and upsamples it through transposed convolutions. This gradually reconstructs the image by increasing its resolution. The skip connections between encoder and decoder layers help retain detailed information from the input, ensuring that the output image has fine features.

**Generates the Output Image**: After the decoding process, the network produces an output image that has the same size as the input image but transformed in style (based on the task, such as generating a colored image from a black-and-white input). The pixel values are scaled to the range [-1, 1] using the tanh activation function.
"""

def define_generator(image_shape=(256,256,3)):
  # weight initialization
  init=RandomNormal(stddev=0.02)
  # image input
  in_image=Input(shape=image_shape)
  # encoder model : C64-C126-C256-C512-C512-C512-C512-C512-C512
  e1=define_encoder_block(in_image,64,batchnorm=False)
  e2=define_encoder_block(e1,128)
  e3=define_encoder_block(e2,256)
  e4=define_encoder_block(e3,512)
  e5=define_encoder_block(e4,512)
  e6=define_encoder_block(e5,512)
  e7=define_encoder_block(e6,512)

  # bottleneck
  b=Conv2D(512,(4,4),strides=(2,2),padding='same',kernel_initializer=init)(e7)
  b=Activation('relu')(b)

  d1=decoder_block(b,e7,512)
  d2=decoder_block(d1,e6,512)
  d3=decoder_block(d2,e5,512)
  d4=decoder_block(d3,e4,512,dropout=False)
  d5=decoder_block(d4,e3,256,dropout=False)
  d6=decoder_block(d5,e2,128,dropout=False)
  d7=decoder_block(d6,e1,64,dropout=False)

  # output
  g=Conv2DTranspose(image_shape[2],(4,4),strides=(2,2),padding='same',kernel_initializer=init)(d7)
  out_image=Activation('tanh')(g)

  model=Model(in_image,out_image)
  return model

g_model=define_generator()
plot_model(g_model,to_file='gen_model.png',show_shapes=True)

"""full GAN model is created using Keras' Model.

It takes the source image (in_src) as input and outputs two things:

dis_out: The discriminator's classification of whether the generated image is real or fake.

"""

def generate_real_samples(dataset,n_samples,patch_shape):

  trainA,trainB=dataset

  ix=np.random.randint(0,trainA.shape[0],n_samples)

  X1,X2=trainA[ix],trainB[ix]

  y=np.ones((n_samples,patch_shape,patch_shape,1))
  return [X1,X2],y

def define_gan(g_model,d_model,image_shape):

  for layer in d_model.layers:
    if not isinstance(layer,BatchNormalization):
      layer.trainable=False

  # Discriminator layers set to untranable in the combined GAN
  # but standalone discriminator will be trainable

  in_src=Input(shape=image_shape)
  gen_out=g_model(in_src)

  dis_out=d_model([in_src,gen_out])


  model=Model(in_src,[dis_out,gen_out])

  opt=Adam(learning_rate=0.0002,beta_1=0.5)
  # Authors suggested weighting BCE vs L1 as 1:100.
  model.compile(loss=['binary_crossentropy','mae'],optimizer=opt,loss_weights=[1,100])

  return model

def generate_fake_samples(g_model,samples,patch_shape):

  X=g_model.predict(samples)

  y=np.zeros((len(X),patch_shape,patch_shape,1))

  return X,y

def train (d_model,g_model,gan_model,dataset,n_epochs=100,n_batch=1):

  n_patch=d_model.output_shape[1]

  trainA,trainB=dataset

  batch_per_epo=int(len(trainA)/n_batch)

  n_steps=batch_per_epo*n_epochs

  for i in range(n_steps):
    # select a batch of real samples
    [X_realA,X_realB],y_real=generate_real_samples(dataset,n_batch,n_patch)
    # generate a batch of fake samples
    X_fakeB,y_fake=generate_fake_samples(g_model,X_realA,n_patch)

    dloss1=d_model.train_on_batch([X_realA,X_realB],y_real)
    dloss2=d_model.train_on_batch([X_realA,X_fakeB],y_fake)

    g_loss,_,_=gan_model.train_on_batch(X_realA,[y_real,X_realB])

    print('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, dloss1, dloss2, g_loss))

    if (i+1)%(batch_per_epo*10)==0:
      summarize_performance(i,g_model,dataset)

def summarize_performance(step, g_model, dataset, n_samples=3):
	# select a sample of input images
	[X_realA, X_realB], _ = generate_real_samples(dataset, n_samples, 1)
	# generate a batch of fake samples
	X_fakeB, _ = generate_fake_samples(g_model, X_realA, 1)
	# scale all pixels from [-1,1] to [0,1]
	X_realA = (X_realA + 1) / 2.0
	X_realB = (X_realB + 1) / 2.0
	X_fakeB = (X_fakeB + 1) / 2.0
	# plot real source images
	for i in range(n_samples):
		plt.subplot(3, n_samples, 1 + i)
		plt.axis('off')
		plt.imshow(X_realA[i])
	# plot generated target image
	for i in range(n_samples):
		plt.subplot(3, n_samples, 1 + n_samples + i)
		plt.axis('off')
		plt.imshow(X_fakeB[i])
	# plot real target image
	for i in range(n_samples):
		plt.subplot(3, n_samples, 1 + n_samples*2 + i)
		plt.axis('off')
		plt.imshow(X_realB[i])
	# save plot to file
	filename1 = 'plot_%06d.png' % (step+1)
	plt.savefig(filename1)
	plt.close()
	# save the generator model
	filename2 = 'model_%06d.h5' % (step+1)
	g_model.save(filename2)
	print('>Saved: %s and %s' % (filename1, filename2))

gan_model=define_gan(g_model,d_model,(256,256,3))
gan_model.compile(loss=['binary_crossentropy','mae'],optimizer=Adam(learning_rate=0.0002,beta_1=0.5),loss_weights=[1,100])

from datetime import datetime
start=datetime.now()

train(d_model,g_model,gan_model,dataset,n_epochs=2,n_batch=1)


stop=datetime.now()

print(stop-start)

from datetime import datetime
start=datetime.now()

train(d_model,g_model,gan_model,dataset,n_epochs=3,n_batch=1)


stop=datetime.now()

summarize_performance(6,g_model,dataset)

from datetime import datetime
start=datetime.now()

train(d_model,g_model,gan_model,dataset,n_epochs=3,n_batch=1)


stop=datetime.now()

summarize_performance(9,g_model,dataset)

